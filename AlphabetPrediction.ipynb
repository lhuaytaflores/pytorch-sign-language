{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91c3a6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version;  2.1.2\n",
      "Torchvision Version:  0.16.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import imutils\n",
    "import cv2\n",
    "import cvzone\n",
    "import warnings\n",
    "import mediapipe as mp\n",
    "\n",
    "from torch import nn\n",
    "from copy import deepcopy\n",
    "from PIL import Image as Imag\n",
    "from torchvision import datasets, models, transforms\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "print(\"PyTorch Version; \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a23de64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "    if model_name == \"DenseNet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "    return model_ft, input_size\n",
    "def probabilidad(frame):\n",
    "    img = frame\n",
    "    img = imutils.resize(img, width=600)\n",
    "    img = cv2.cvtColor(deepcopy(img), cv2.COLOR_BGR2RGB)\n",
    "    image_pil = Imag.fromarray(img)\n",
    "    input_size = 224\n",
    "    data_transform = transforms.Compose([\n",
    "      transforms.Resize(input_size),\n",
    "      transforms.CenterCrop(input_size),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    img = data_transform(image_pil)\n",
    "    img = img.unsqueeze(0).to(device)\n",
    "    out = model_ft(img).cpu().detach().tolist()[0]\n",
    "    A_L, A_R, B_L, B_R, C_L, C_R, D_L, D_R, E_L, E_R, F_L, F_R, G_L, G_R, H_L, H_R, I_L, I_R, J_L, J_R, K_L, K_R, L_L, L_R, M_L, M_R, N_L, N_R, O_L, O_R, P_L, P_R, Q_L, Q_R, R_L, R_R, S_L, S_R, T_L, T_R, U_L, U_R, V_L, V_R, W_L, W_R, X_L, X_R, Y_L, Y_R, Z_L, Z_R = torch.nn.functional.softmax(torch.tensor(out)).tolist()\n",
    "    symbols = {A_L:\"A\", A_R:\"A\", B_L:\"B\", B_R:\"B\", C_L:\"C\", C_R:\"C\", D_L:\"D\", D_R:\"D\", E_L:\"E\", E_R:\"E\", F_L:\"F\", F_R:\"F\", G_L:\"G\", G_R:\"G\", H_L:\"H\", H_R:\"H\", I_L:\"I\", I_R:\"I\", J_L:\"J\", J_R:\"J\", K_L:\"K\", K_R:\"K\", L_L:\"L\", L_R:\"L\", M_L:\"M\", M_R:\"M\", N_L:\"N\", N_R:\"N\", O_L:\"O\", O_R:\"O\", P_L:\"P\", P_R:\"P\", Q_L:\"Q\", Q_R:\"Q\", R_L:\"R\", R_R:\"R\", S_L:\"S\", S_R:\"S\", T_L:\"T\", T_R:\"T\", U_L:\"U\", U_R:\"U\", V_L:\"V\", V_R:\"V\", W_L:\"W\", W_R:\"W\", X_L:\"X\", X_R:\"X\", Y_L:\"Y\", Y_R:\"Y\", Z_L:\"Z\", Z_R:\"Z\"}\n",
    "    maxim = max(torch.nn.functional.softmax(torch.tensor(out)).tolist())\n",
    "    labels = str(symbols.get(maxim))\n",
    "    return maxim, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5100be28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft, input_size = initialize_model(\"DenseNet\", 52, False, use_pretrained=True)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_ft.eval()\n",
    "model_ft.to(device)\n",
    "model_ft.load_state_dict(torch.load('D:/CPU/best.pt')['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "315a6c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] starting video stream...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'hand' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hands:\n\u001b[0;32m     10\u001b[0m     hand1 \u001b[38;5;241m=\u001b[39m hands[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hand1 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m hand[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m     12\u001b[0m         bbox \u001b[38;5;241m=\u001b[39m hand1[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbbox\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     13\u001b[0m         x, y, ancho, alto \u001b[38;5;241m=\u001b[39m bbox\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hand' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] starting video stream...\")\n",
    "cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "#cap = cv2.VideoCapture('D:/Lenguaje de seÃ±as/A.mp4')\n",
    "detector = HandDetector(detectionCon=0.8,maxHands=1)\n",
    "i=0\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    hands = detector.findHands(img, draw=False)\n",
    "    if hands:\n",
    "        hand1 = hands[0]\n",
    "        if hand1 and \"bbox\" in hand1[1]:\n",
    "            bbox = hand1[0]['bbox']\n",
    "            x, y, ancho, alto = bbox\n",
    "            x1, y1, x2, y2 = x, y, ancho, alto\n",
    "            x1, y1 = max(x1,0), max(y1,0)\n",
    "            \n",
    "            img = cvzone.cornerRect(img, (x1-20, y1-20, x2+40, y2+40))\n",
    "            \n",
    "            maximo, etiqueta = probabilidad(img)\n",
    "            maximo = str(maximo*100)+str('%')\n",
    "            \n",
    "            t1 = x1 - 25, y1 - 70\n",
    "            t2 = x1 - 25, y1 - 40\n",
    "            \n",
    "            cv2.putText(img, etiqueta, (int(t1[0]), int(t1[1])),cv2.FONT_HERSHEY_SIMPLEX, 1,(0,255,0),2)\n",
    "            cv2.putText(img, maximo, (int(t2[0]), int(t2[1])),cv2.FONT_HERSHEY_SIMPLEX, 1,(0,255,0),2)\n",
    "    \n",
    "    cv2.imshow(\"Image\", img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "#pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223e1298",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "934bed29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "def indice_camara():\n",
    "    index = -2\n",
    "    arr = []\n",
    "    i = 10\n",
    "    while i > 0:\n",
    "        cap = cv2.VideoCapture(index)\n",
    "        if cap.read()[0]:\n",
    "            arr.append(index)\n",
    "            cap.release()\n",
    "        index += 1\n",
    "        i -= 1\n",
    "    return arr\n",
    "print(indice_camara())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1017d78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0);\n",
    "\n",
    "while(True):\n",
    "    #Captura frame-by-frame\n",
    "    ret, frame = cap.read();\n",
    "\n",
    "    #Muestra el resultado de frame\n",
    "    cv2.imshow('frame',frame)\n",
    "    if cv2.waitKey(20) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "#Cuando todo este hecho cierra el programa\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53664a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
